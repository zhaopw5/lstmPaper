{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5b67bd-cf05-4a2f-8f80-21ddbb4b7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SEQUENCE_LENGTH, TRAIN_RATIO, VAL_RATIO, TEST_RATIO, SOLAR_DATA_PATH, COSMIC_DATA_PATH, RESULTS_DIR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae9a466-8372-4b80-bb97-babb188568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to use\n",
    "SOLAR_PARAMETERS = ['HMF', 'wind_speed', 'HCS_tilt', 'polarity', 'SSN', 'daily_OSF']\n",
    "PROTON_FLUX_COL = 'proton_flux m^-2sr^-1s^-1GV^-1'\n",
    "\n",
    "# rigidity bins in GV\n",
    "RIGIDITY_BIN_EDGES = [1, 1.16, 1.33, 1.51, 1.71, 1.92, 2.15, 2.4, 2.67, 2.97, 3.29, 3.64, 4.02, \n",
    "                      4.43, 4.88, 5.37, 5.9, 6.47, 7.09, 7.76, 8.48, 9.26, 10.1]\n",
    "# rigidity_min GV\n",
    "RIGIDITY_VALUES = RIGIDITY_BIN_EDGES[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3812a4-5776-446d-8e9a-d48fa55afd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_check_data():\n",
    "  \n",
    "    \"\"\"读数据，检查缺失/重复日期，插值补全\"\"\"\n",
    "    # 太阳数据\n",
    "    solar_data = pd.read_csv(SOLAR_DATA_PATH)\n",
    "    solar_data['date'] = pd.to_datetime(solar_data['date'])\n",
    "    \n",
    "    # 宇宙线数据\n",
    "    cosmic_data = pd.read_csv(COSMIC_DATA_PATH)\n",
    "    cosmic_data['date YYYY-MM-DD'] = pd.to_datetime(cosmic_data['date YYYY-MM-DD'])\n",
    "    \n",
    "    # 筛选所需的刚度数据并重组\n",
    "    cosmic_multi_rigidity = []\n",
    "    for rigidity in RIGIDITY_VALUES:\n",
    "        rigidity_data = cosmic_data[cosmic_data['rigidity_min GV'] == rigidity].copy()\n",
    "        # if len(rigidity_data) > 0:\n",
    "        rigidity_data = rigidity_data[['date YYYY-MM-DD', PROTON_FLUX_COL]].copy()\n",
    "        rigidity_data = rigidity_data.rename(columns={PROTON_FLUX_COL: f'proton_{rigidity}GV'})\n",
    "        cosmic_multi_rigidity.append(rigidity_data)\n",
    "        # else:\n",
    "        #     print(f\"警告: 刚度 {rigidity} GV 没有数据\")\n",
    "\n",
    "    # 合并所有刚度数据\n",
    "    # if cosmic_multi_rigidity:\n",
    "    cosmic_data = cosmic_multi_rigidity[0]\n",
    "    for i in range(1, len(cosmic_multi_rigidity)):\n",
    "        cosmic_data = cosmic_data.merge(cosmic_multi_rigidity[i], on='date YYYY-MM-DD', how='outer')\n",
    "    # else:\n",
    "    #     raise ValueError(\"没有找到任何刚度数据\")\n",
    "\n",
    "    # 更新cosmic ray列名列表\n",
    "    proton_flux_cols = [f'proton_{rigidity}GV' for rigidity in RIGIDITY_VALUES if f'proton_{rigidity}GV' in cosmic_data.columns]\n",
    "    print(f\"成功加载 {len(proton_flux_cols)} 个刚度的数据: {[col.split('_')[-1] for col in proton_flux_cols]}\")\n",
    "\n",
    "    # --- Debug alignment information ---\n",
    "    print(\"=== Data Alignment Debug ===\")\n",
    "    # Print ranges\n",
    "    print(f\"Solar data range: {solar_data['date'].min()} to {solar_data['date'].max()}\")\n",
    "    print(f\"Cosmic data range: {cosmic_data['date YYYY-MM-DD'].min()} to {cosmic_data['date YYYY-MM-DD'].max()}\")\n",
    "    \n",
    "    # Check total counts\n",
    "    print(f\"Total solar days: {len(solar_data)}\")\n",
    "    print(f\"Total cosmic days before interpolation: {len(cosmic_data)}\")\n",
    "\n",
    "    # 检查数据中是否有缺失日期\n",
    "    solar_dates = pd.to_datetime(solar_data['date'])\n",
    "    cosmic_dates = pd.to_datetime(cosmic_data['date YYYY-MM-DD'])\n",
    "    full_solar = pd.date_range(start=solar_dates.min(), end=solar_dates.max(), freq='D')\n",
    "    full_cosmic = pd.date_range(start=cosmic_dates.min(), end=cosmic_dates.max(), freq='D')\n",
    "    missing_solar = set(full_solar) - set(solar_dates)\n",
    "    missing_cosmic = set(full_cosmic) - set(cosmic_dates)\n",
    "    print(f\"Missing solar days: {len(missing_solar)}\")\n",
    "    if missing_solar:\n",
    "        first5 = sorted(list(missing_solar))[:5]\n",
    "        print(f\"First 5 missing solar dates: {first5}\")\n",
    "    print(f\"Missing cosmic days: {len(missing_cosmic)}\")\n",
    "    if missing_cosmic:\n",
    "        first5_cos = sorted(list(missing_cosmic))[:5]\n",
    "        print(f\"First 5 missing cosmic dates: {first5_cos}\")\n",
    "\n",
    "    # 检查数据中是否有重复的日期\n",
    "    solar_dups = solar_data[solar_data.duplicated('date', keep=False)]\n",
    "    cosmic_dups = cosmic_data[cosmic_data.duplicated('date YYYY-MM-DD', keep=False)]\n",
    "    print(f\"Duplicate solar dates: {len(solar_dups)}\")\n",
    "    print(f\"Duplicate cosmic dates: {len(cosmic_dups)}\")\n",
    "\n",
    "    ##########################################################################\n",
    "    # --- 分别插值补全太阳数据和宇宙线数据 ---\n",
    "    print(\"=== 分别插值补全数据 ===\")\n",
    "    \n",
    "    # 插值补全太阳数据 - 使用太阳数据自己的时间范围\n",
    "    print(\"Interpolating solar data...\")\n",
    "    solar_full_range = pd.date_range(start=solar_dates.min(), end=solar_dates.max(), freq='D')\n",
    "    print(f\"Solar interpolation range: {solar_full_range[0]} to {solar_full_range[-1]} ({len(solar_full_range)} days)\")\n",
    "    \n",
    "    solar_data = solar_data.set_index('date').reindex(solar_full_range)\n",
    "    for col in SOLAR_PARAMETERS:\n",
    "        if col in solar_data.columns:\n",
    "            solar_data[col] = solar_data[col].interpolate(method='linear')\n",
    "    solar_data = solar_data.reset_index().rename(columns={'index': 'date'})\n",
    "    print(f\"Total solar days after interpolation: {len(solar_data)}\")\n",
    "    \n",
    "    # 插值补全宇宙线数据 - 使用宇宙线数据自己的时间范围\n",
    "    print(\"Interpolating cosmic data...\")\n",
    "    cosmic_full_range = pd.date_range(start=cosmic_dates.min(), end=cosmic_dates.max(), freq='D')\n",
    "    print(f\"Cosmic interpolation range: {cosmic_full_range[0]} to {cosmic_full_range[-1]} ({len(cosmic_full_range)} days)\")\n",
    "    \n",
    "    cosmic_data = cosmic_data.set_index('date YYYY-MM-DD').reindex(cosmic_full_range)\n",
    "    for col in proton_flux_cols:\n",
    "        if col in cosmic_data.columns:\n",
    "            cosmic_data[col] = cosmic_data[col].interpolate(method='linear')\n",
    "    cosmic_data = cosmic_data.reset_index().rename(columns={'index': 'date YYYY-MM-DD'})\n",
    "    print(f\"Total cosmic days after interpolation: {len(cosmic_data)}\")\n",
    "    \n",
    "    print(\"太阳和宇宙线数据已分别按各自时间范围线性插值补全完成。\")\n",
    "    \n",
    "    # 确保results目录存在\n",
    "    results_dir = Path(RESULTS_DIR)\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 保存插值后的数据到results目录\n",
    "    cosmic_data.to_csv(results_dir / \"interpolated_cosmic_data.csv\", index=False)\n",
    "    solar_data.to_csv(results_dir / \"interpolated_solar_data.csv\", index=False)\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n=== Final data summary ===\")\n",
    "    print(f\"Solar data: {len(solar_data)} days ({solar_data['date'].min()} - {solar_data['date'].max()})\")\n",
    "    print(f\"Cosmic data: {len(cosmic_data)} days ({cosmic_data['date YYYY-MM-DD'].min()} - {cosmic_data['date YYYY-MM-DD'].max()})\")\n",
    "\n",
    "    return solar_data, cosmic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a272fc-4cc8-4c0d-b31e-1066c1037cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>HMF</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>HCS_tilt</th>\n",
       "      <th>polarity</th>\n",
       "      <th>SSN</th>\n",
       "      <th>daily_OSF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>6.2</td>\n",
       "      <td>701.0</td>\n",
       "      <td>11.35</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.521234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985-01-02</td>\n",
       "      <td>5.7</td>\n",
       "      <td>650.0</td>\n",
       "      <td>11.25</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.252387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985-01-03</td>\n",
       "      <td>5.5</td>\n",
       "      <td>551.0</td>\n",
       "      <td>11.15</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.332165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985-01-04</td>\n",
       "      <td>5.3</td>\n",
       "      <td>452.0</td>\n",
       "      <td>11.06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.411943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985-01-05</td>\n",
       "      <td>6.9</td>\n",
       "      <td>421.0</td>\n",
       "      <td>10.96</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.963117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14599</th>\n",
       "      <td>2024-12-21</td>\n",
       "      <td>8.5</td>\n",
       "      <td>511.0</td>\n",
       "      <td>63.29</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>164</td>\n",
       "      <td>11.654946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14600</th>\n",
       "      <td>2024-12-22</td>\n",
       "      <td>7.3</td>\n",
       "      <td>612.0</td>\n",
       "      <td>63.26</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>183</td>\n",
       "      <td>12.229319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14601</th>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>6.3</td>\n",
       "      <td>565.0</td>\n",
       "      <td>63.23</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>224</td>\n",
       "      <td>10.625598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14602</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>5.3</td>\n",
       "      <td>549.0</td>\n",
       "      <td>63.20</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>223</td>\n",
       "      <td>8.126296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14603</th>\n",
       "      <td>2024-12-25</td>\n",
       "      <td>4.2</td>\n",
       "      <td>478.0</td>\n",
       "      <td>63.14</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>241</td>\n",
       "      <td>4.958957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14604 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  HMF  wind_speed  HCS_tilt  polarity  SSN  daily_OSF\n",
       "0     1985-01-01  6.2       701.0     11.35      -1.0    0   9.521234\n",
       "1     1985-01-02  5.7       650.0     11.25      -1.0    0   6.252387\n",
       "2     1985-01-03  5.5       551.0     11.15      -1.0    0   9.332165\n",
       "3     1985-01-04  5.3       452.0     11.06      -1.0    0  12.411943\n",
       "4     1985-01-05  6.9       421.0     10.96      -1.0    0  15.963117\n",
       "...          ...  ...         ...       ...       ...  ...        ...\n",
       "14599 2024-12-21  8.5       511.0     63.29      -1.0  164  11.654946\n",
       "14600 2024-12-22  7.3       612.0     63.26      -1.0  183  12.229319\n",
       "14601 2024-12-23  6.3       565.0     63.23      -1.0  224  10.625598\n",
       "14602 2024-12-24  5.3       549.0     63.20      -1.0  223   8.126296\n",
       "14603 2024-12-25  4.2       478.0     63.14      -1.0  241   4.958957\n",
       "\n",
       "[14604 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b00c5a00-5e3b-4135-bd05-49314578e144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date YYYY-MM-DD</th>\n",
       "      <th>proton_1GV</th>\n",
       "      <th>proton_1.16GV</th>\n",
       "      <th>proton_1.33GV</th>\n",
       "      <th>proton_1.51GV</th>\n",
       "      <th>proton_1.71GV</th>\n",
       "      <th>proton_1.92GV</th>\n",
       "      <th>proton_2.15GV</th>\n",
       "      <th>proton_2.4GV</th>\n",
       "      <th>proton_2.67GV</th>\n",
       "      <th>...</th>\n",
       "      <th>proton_4.02GV</th>\n",
       "      <th>proton_4.43GV</th>\n",
       "      <th>proton_4.88GV</th>\n",
       "      <th>proton_5.37GV</th>\n",
       "      <th>proton_5.9GV</th>\n",
       "      <th>proton_6.47GV</th>\n",
       "      <th>proton_7.09GV</th>\n",
       "      <th>proton_7.76GV</th>\n",
       "      <th>proton_8.48GV</th>\n",
       "      <th>proton_9.26GV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-05-20</td>\n",
       "      <td>999.8</td>\n",
       "      <td>974.9</td>\n",
       "      <td>914.4</td>\n",
       "      <td>840.4</td>\n",
       "      <td>739.4</td>\n",
       "      <td>630.2</td>\n",
       "      <td>548.9</td>\n",
       "      <td>462.8</td>\n",
       "      <td>392.7</td>\n",
       "      <td>...</td>\n",
       "      <td>184.4</td>\n",
       "      <td>150.0</td>\n",
       "      <td>121.8</td>\n",
       "      <td>98.97</td>\n",
       "      <td>79.75</td>\n",
       "      <td>64.81</td>\n",
       "      <td>51.83</td>\n",
       "      <td>41.23</td>\n",
       "      <td>33.92</td>\n",
       "      <td>26.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-05-21</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>927.7</td>\n",
       "      <td>839.6</td>\n",
       "      <td>747.7</td>\n",
       "      <td>648.3</td>\n",
       "      <td>554.0</td>\n",
       "      <td>468.6</td>\n",
       "      <td>394.6</td>\n",
       "      <td>...</td>\n",
       "      <td>184.6</td>\n",
       "      <td>149.6</td>\n",
       "      <td>122.1</td>\n",
       "      <td>97.74</td>\n",
       "      <td>78.28</td>\n",
       "      <td>63.01</td>\n",
       "      <td>50.56</td>\n",
       "      <td>40.95</td>\n",
       "      <td>32.60</td>\n",
       "      <td>26.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-05-22</td>\n",
       "      <td>991.9</td>\n",
       "      <td>973.6</td>\n",
       "      <td>888.3</td>\n",
       "      <td>818.4</td>\n",
       "      <td>725.6</td>\n",
       "      <td>626.5</td>\n",
       "      <td>546.9</td>\n",
       "      <td>457.4</td>\n",
       "      <td>387.9</td>\n",
       "      <td>...</td>\n",
       "      <td>181.5</td>\n",
       "      <td>148.6</td>\n",
       "      <td>121.0</td>\n",
       "      <td>98.81</td>\n",
       "      <td>79.00</td>\n",
       "      <td>64.89</td>\n",
       "      <td>51.60</td>\n",
       "      <td>41.73</td>\n",
       "      <td>33.23</td>\n",
       "      <td>26.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-05-23</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>987.2</td>\n",
       "      <td>905.4</td>\n",
       "      <td>833.2</td>\n",
       "      <td>731.1</td>\n",
       "      <td>652.2</td>\n",
       "      <td>551.5</td>\n",
       "      <td>470.1</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>184.5</td>\n",
       "      <td>149.4</td>\n",
       "      <td>121.7</td>\n",
       "      <td>99.34</td>\n",
       "      <td>79.74</td>\n",
       "      <td>63.99</td>\n",
       "      <td>51.85</td>\n",
       "      <td>41.41</td>\n",
       "      <td>33.01</td>\n",
       "      <td>27.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>999.7</td>\n",
       "      <td>924.2</td>\n",
       "      <td>835.2</td>\n",
       "      <td>748.2</td>\n",
       "      <td>650.1</td>\n",
       "      <td>549.2</td>\n",
       "      <td>470.8</td>\n",
       "      <td>390.8</td>\n",
       "      <td>...</td>\n",
       "      <td>185.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>122.2</td>\n",
       "      <td>98.79</td>\n",
       "      <td>79.81</td>\n",
       "      <td>64.34</td>\n",
       "      <td>51.86</td>\n",
       "      <td>41.75</td>\n",
       "      <td>33.06</td>\n",
       "      <td>26.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>874.8</td>\n",
       "      <td>739.0</td>\n",
       "      <td>601.4</td>\n",
       "      <td>494.5</td>\n",
       "      <td>...</td>\n",
       "      <td>213.6</td>\n",
       "      <td>171.6</td>\n",
       "      <td>138.5</td>\n",
       "      <td>110.90</td>\n",
       "      <td>88.85</td>\n",
       "      <td>70.60</td>\n",
       "      <td>56.27</td>\n",
       "      <td>44.67</td>\n",
       "      <td>35.79</td>\n",
       "      <td>28.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>1221.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>869.8</td>\n",
       "      <td>725.2</td>\n",
       "      <td>595.6</td>\n",
       "      <td>489.8</td>\n",
       "      <td>...</td>\n",
       "      <td>214.2</td>\n",
       "      <td>172.5</td>\n",
       "      <td>137.4</td>\n",
       "      <td>110.90</td>\n",
       "      <td>88.60</td>\n",
       "      <td>70.56</td>\n",
       "      <td>56.33</td>\n",
       "      <td>44.97</td>\n",
       "      <td>35.76</td>\n",
       "      <td>28.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>2019-10-27</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>868.2</td>\n",
       "      <td>725.2</td>\n",
       "      <td>597.0</td>\n",
       "      <td>485.4</td>\n",
       "      <td>...</td>\n",
       "      <td>212.5</td>\n",
       "      <td>170.6</td>\n",
       "      <td>137.2</td>\n",
       "      <td>109.80</td>\n",
       "      <td>87.53</td>\n",
       "      <td>70.37</td>\n",
       "      <td>56.10</td>\n",
       "      <td>44.47</td>\n",
       "      <td>35.49</td>\n",
       "      <td>28.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>1383.0</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>863.4</td>\n",
       "      <td>719.4</td>\n",
       "      <td>592.6</td>\n",
       "      <td>484.6</td>\n",
       "      <td>...</td>\n",
       "      <td>210.6</td>\n",
       "      <td>171.1</td>\n",
       "      <td>136.4</td>\n",
       "      <td>109.70</td>\n",
       "      <td>88.10</td>\n",
       "      <td>69.96</td>\n",
       "      <td>56.25</td>\n",
       "      <td>44.86</td>\n",
       "      <td>35.77</td>\n",
       "      <td>28.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>863.1</td>\n",
       "      <td>713.7</td>\n",
       "      <td>592.4</td>\n",
       "      <td>484.9</td>\n",
       "      <td>...</td>\n",
       "      <td>208.9</td>\n",
       "      <td>171.1</td>\n",
       "      <td>135.7</td>\n",
       "      <td>110.60</td>\n",
       "      <td>88.54</td>\n",
       "      <td>70.54</td>\n",
       "      <td>55.96</td>\n",
       "      <td>44.98</td>\n",
       "      <td>35.48</td>\n",
       "      <td>28.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2824 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date YYYY-MM-DD  proton_1GV  proton_1.16GV  proton_1.33GV  proton_1.51GV  \\\n",
       "0         2011-05-20       999.8          974.9          914.4          840.4   \n",
       "1         2011-05-21      1012.0         1005.0          927.7          839.6   \n",
       "2         2011-05-22       991.9          973.6          888.3          818.4   \n",
       "3         2011-05-23      1009.0          987.2          905.4          833.2   \n",
       "4         2011-05-24      1046.0          999.7          924.2          835.2   \n",
       "...              ...         ...            ...            ...            ...   \n",
       "2819      2019-10-25      1756.0         1592.0         1409.0         1221.0   \n",
       "2820      2019-10-26      1710.0         1559.0         1395.0         1221.0   \n",
       "2821      2019-10-27      1668.0         1526.0         1385.0         1209.0   \n",
       "2822      2019-10-28      1665.0         1547.0         1383.0         1207.0   \n",
       "2823      2019-10-29      1687.0         1540.0         1374.0         1204.0   \n",
       "\n",
       "      proton_1.71GV  proton_1.92GV  proton_2.15GV  proton_2.4GV  \\\n",
       "0             739.4          630.2          548.9         462.8   \n",
       "1             747.7          648.3          554.0         468.6   \n",
       "2             725.6          626.5          546.9         457.4   \n",
       "3             731.1          652.2          551.5         470.1   \n",
       "4             748.2          650.1          549.2         470.8   \n",
       "...             ...            ...            ...           ...   \n",
       "2819         1039.0          874.8          739.0         601.4   \n",
       "2820         1040.0          869.8          725.2         595.6   \n",
       "2821         1031.0          868.2          725.2         597.0   \n",
       "2822         1028.0          863.4          719.4         592.6   \n",
       "2823         1029.0          863.1          713.7         592.4   \n",
       "\n",
       "      proton_2.67GV  ...  proton_4.02GV  proton_4.43GV  proton_4.88GV  \\\n",
       "0             392.7  ...          184.4          150.0          121.8   \n",
       "1             394.6  ...          184.6          149.6          122.1   \n",
       "2             387.9  ...          181.5          148.6          121.0   \n",
       "3             394.0  ...          184.5          149.4          121.7   \n",
       "4             390.8  ...          185.0          151.0          122.2   \n",
       "...             ...  ...            ...            ...            ...   \n",
       "2819          494.5  ...          213.6          171.6          138.5   \n",
       "2820          489.8  ...          214.2          172.5          137.4   \n",
       "2821          485.4  ...          212.5          170.6          137.2   \n",
       "2822          484.6  ...          210.6          171.1          136.4   \n",
       "2823          484.9  ...          208.9          171.1          135.7   \n",
       "\n",
       "      proton_5.37GV  proton_5.9GV  proton_6.47GV  proton_7.09GV  \\\n",
       "0             98.97         79.75          64.81          51.83   \n",
       "1             97.74         78.28          63.01          50.56   \n",
       "2             98.81         79.00          64.89          51.60   \n",
       "3             99.34         79.74          63.99          51.85   \n",
       "4             98.79         79.81          64.34          51.86   \n",
       "...             ...           ...            ...            ...   \n",
       "2819         110.90         88.85          70.60          56.27   \n",
       "2820         110.90         88.60          70.56          56.33   \n",
       "2821         109.80         87.53          70.37          56.10   \n",
       "2822         109.70         88.10          69.96          56.25   \n",
       "2823         110.60         88.54          70.54          55.96   \n",
       "\n",
       "      proton_7.76GV  proton_8.48GV  proton_9.26GV  \n",
       "0             41.23          33.92          26.69  \n",
       "1             40.95          32.60          26.28  \n",
       "2             41.73          33.23          26.75  \n",
       "3             41.41          33.01          27.01  \n",
       "4             41.75          33.06          26.72  \n",
       "...             ...            ...            ...  \n",
       "2819          44.67          35.79          28.50  \n",
       "2820          44.97          35.76          28.36  \n",
       "2821          44.47          35.49          28.37  \n",
       "2822          44.86          35.77          28.37  \n",
       "2823          44.98          35.48          28.37  \n",
       "\n",
       "[2824 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8bca3f-22a7-4076-acad-5b598a55afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(solar_data, cosmic_data, sequence_length=SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    每个样本输入：过去SEQUENCE_LENGTH天的[太阳参数*len(SOLAR_PARAMETERS) + cosmic_ray*len(RIGIDITY_VALUES)]，\n",
    "    输出：第SEQUENCE_LENGTH+1天的所有刚度的cosmic_ray\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== 创建 {sequence_length} 天序列（太阳参数+多刚度宇宙线流强） ===\")\n",
    "    proton_flux_cols = [f'proton_{rigidity}GV' for rigidity in RIGIDITY_VALUES if f'proton_{rigidity}GV' in cosmic_data.columns]\n",
    "    features = SOLAR_PARAMETERS + proton_flux_cols\n",
    "    print(f\"输入特征: {features}\")\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    dates = []\n",
    "    successful_alignments = 0\n",
    "    failed_alignments = 0\n",
    "    \n",
    "    for idx in range(len(cosmic_data) - sequence_length):\n",
    "        # 输入窗口的起止日期\n",
    "        input_start = cosmic_data.loc[idx, 'date YYYY-MM-DD']\n",
    "        input_end = cosmic_data.loc[idx + sequence_length - 1, 'date YYYY-MM-DD']\n",
    "        output_date = cosmic_data.loc[idx + sequence_length, 'date YYYY-MM-DD']\n",
    "        \n",
    "        # 构造输入序列\n",
    "        input_rows = []\n",
    "        for i in range(sequence_length):\n",
    "            date_i = cosmic_data.loc[idx + i, 'date YYYY-MM-DD']\n",
    "            # 查找太阳参数\n",
    "            solar_mask = solar_data['date'] == date_i\n",
    "            if solar_mask.sum() == 1:\n",
    "                solar_row = solar_data[solar_mask][SOLAR_PARAMETERS].iloc[0].values\n",
    "                # 获取所有刚度的cosmic ray数据\n",
    "                proton_flux_row = cosmic_data.loc[idx + i, proton_flux_cols].values\n",
    "                input_rows.append(np.concatenate([solar_row, proton_flux_row]))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if len(input_rows) == sequence_length:\n",
    "            X.append(np.array(input_rows))\n",
    "            # 输出是所有刚度的cosmic ray\n",
    "            y.append(cosmic_data.loc[idx + sequence_length, proton_flux_cols].values)\n",
    "            dates.append(output_date)\n",
    "            successful_alignments += 1\n",
    "            if successful_alignments <= 3:\n",
    "                print(f\"\\n样例 {successful_alignments}: 输入 {input_start} 到 {input_end}, 输出 {output_date}\")\n",
    "                print(f\"  输入形状: {np.array(input_rows).shape}\")\n",
    "                print(f\"  输出形状: {len(proton_flux_cols)} 个刚度\")\n",
    "        else:\n",
    "            failed_alignments += 1\n",
    "            if failed_alignments <= 3:\n",
    "                print(f\"失败样例 {failed_alignments}: 只找到 {len(input_rows)} 天数据，需要 {sequence_length} 天\")\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f\"\\n=== 序列创建结果 ===\")\n",
    "    print(f\"成功对齐: {successful_alignments} 个样例\")\n",
    "    print(f\"失败对齐: {failed_alignments} 个样例\")\n",
    "    print(f\"最终数据形状:\")\n",
    "    print(f\"  X: {X.shape} (样例数, 时间步数, 特征数)\")\n",
    "    print(f\"  y: {y.shape} (样例数, 刚度数)\")\n",
    "    print(f\"  特征顺序: {features}\")\n",
    "    print(f\"  刚度输出顺序: {proton_flux_cols}\")\n",
    "    \n",
    "    print(f\"\\n=== 数据质量检查 ===\")\n",
    "    if X.shape[0] > 0:\n",
    "        # 转换为numpy数组并检查NaN\n",
    "        X_array = np.array(X, dtype=np.float64)\n",
    "        y_array = np.array(y, dtype=np.float64)\n",
    "        \n",
    "        print(f\"X 中的 NaN 数量: {np.isnan(X_array).sum()}\")\n",
    "        print(f\"y 中的 NaN 数量: {np.isnan(y_array).sum()}\")\n",
    "        \n",
    "        print(f\"\\nX 统计 (所有特征):\")\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_data = X_array[:, :, i].flatten()\n",
    "            if not np.all(np.isnan(feature_data)):\n",
    "                print(f\"  {feature}: 均值={np.nanmean(feature_data):.4f}, 标准差={np.nanstd(feature_data):.4f}, 范围=[{np.nanmin(feature_data):.2f}, {np.nanmax(feature_data):.2f}]\")\n",
    "            else:\n",
    "                print(f\"  {feature}: 全部为NaN\")\n",
    "        \n",
    "        print(f\"\\ny 统计 (所有刚度):\")\n",
    "        for i, rigidity in enumerate(proton_flux_cols):\n",
    "            rigidity_data = y_array[:, i].flatten()\n",
    "            if not np.all(np.isnan(rigidity_data)):\n",
    "                print(f\"  {rigidity}: 均值={np.nanmean(rigidity_data):.4f}, 标准差={np.nanstd(rigidity_data):.4f}, 范围=[{np.nanmin(rigidity_data):.2f}, {np.nanmax(rigidity_data):.2f}]\")\n",
    "            else:\n",
    "                print(f\"  {rigidity}: 全部为NaN\")\n",
    "    else:\n",
    "        print(\"没有数据可检查\")\n",
    "    \n",
    "    return X, y, dates\n",
    "\n",
    "\n",
    "def split_data_three_way(X, y, dates, \n",
    "                        train_ratio=TRAIN_RATIO, \n",
    "                        val_ratio=VAL_RATIO, \n",
    "                        test_ratio=TEST_RATIO):\n",
    "    \"\"\"标准三分法数据划分（使用配置中的比例）\"\"\"\n",
    "    print(f\"=== 标准三分法数据划分 ===\")\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    n_train = int(n_samples * train_ratio)\n",
    "    n_val = int(n_samples * val_ratio)\n",
    "    n_test = n_samples - n_train - n_val  # 确保所有样本都被使用\n",
    "    \n",
    "    print(f\"总样本数: {n_samples}\")\n",
    "    print(f\"  训练集: {n_train} 样例 ({train_ratio*100:.1f}%)\")\n",
    "    print(f\"  验证集: {n_val} 样例 ({val_ratio*100:.1f}%)\")\n",
    "    print(f\"  测试集: {n_test} 样例 ({(n_test/n_samples)*100:.1f}%)\")\n",
    "    \n",
    "    # 按时间顺序划分\n",
    "    X_train = X[:n_train]\n",
    "    X_val = X[n_train:n_train+n_val]\n",
    "    X_test = X[n_train+n_val:]\n",
    "    \n",
    "    y_train = y[:n_train]\n",
    "    y_val = y[n_train:n_train+n_val]\n",
    "    y_test = y[n_train+n_val:]\n",
    "    \n",
    "    dates_train = dates[:n_train]\n",
    "    dates_val = dates[n_train:n_train+n_val]\n",
    "    dates_test = dates[n_train+n_val:]\n",
    "    \n",
    "    print(f\"  训练时间范围: {dates_train[0]} 到 {dates_train[-1]}\")\n",
    "    print(f\"  验证时间范围: {dates_val[0]} 到 {dates_val[-1]}\")\n",
    "    print(f\"  测试时间范围: {dates_test[0]} 到 {dates_test[-1]}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, dates_train, dates_val, dates_test\n",
    "\n",
    "\n",
    "def normalize_data_three_way(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"三分法数据归一化 - 只用训练集拟合归一化器\"\"\"\n",
    "    print(f\"\\n=== 三分法数据归一化 ===\")\n",
    "    \n",
    "    # 打印归一化前的统计\n",
    "    print(f\"归一化前:\")\n",
    "    print(f\"  X_train 形状: {X_train.shape}\")\n",
    "    print(f\"  X_val 形状: {X_val.shape}\")\n",
    "    print(f\"  X_test 形状: {X_test.shape}\")\n",
    "    print(f\"  y_train 形状: {y_train.shape}\")\n",
    "    \n",
    "    # 确保数据类型正确并计算统计量\n",
    "    y_train_array = np.asarray(y_train, dtype=np.float64)\n",
    "    y_mean = np.mean(y_train_array, axis=0)\n",
    "    y_std = np.std(y_train_array, axis=0)\n",
    "    print(f\"  y_train 统计: 均值={y_mean[:3]}..., 标准差={y_std[:3]}...\")  # 只显示前3个以节省空间    \n",
    "    # 对输入数据进行归一化 (重塑为二维进行归一化)\n",
    "    n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "    n_samples_val = X_val.shape[0]\n",
    "    n_samples_test = X_test.shape[0]\n",
    "    \n",
    "    # 重塑为二维\n",
    "    X_train_2d = X_train.reshape(-1, n_features)\n",
    "    X_val_2d = X_val.reshape(-1, n_features)\n",
    "    X_test_2d = X_test.reshape(-1, n_features)\n",
    "    \n",
    "    # 使用StandardScaler - 只用训练集拟合\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled_2d = scaler_X.fit_transform(X_train_2d)\n",
    "    X_val_scaled_2d = scaler_X.transform(X_val_2d)\n",
    "    X_test_scaled_2d = scaler_X.transform(X_test_2d)\n",
    "    \n",
    "    # 重塑回三维\n",
    "    X_train_scaled = X_train_scaled_2d.reshape(n_samples_train, n_timesteps, n_features)\n",
    "    X_val_scaled = X_val_scaled_2d.reshape(n_samples_val, n_timesteps, n_features)\n",
    "    X_test_scaled = X_test_scaled_2d.reshape(n_samples_test, n_timesteps, n_features)\n",
    "    \n",
    "    # 对输出数据进行归一化 - 处理多维输出，只用训练集拟合\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    # 打印归一化后的统计\n",
    "    print(f\"归一化后:\")\n",
    "    print(f\"  X_train_scaled 统计: 均值={np.mean(X_train_scaled):.4f}, 标准差={np.std(X_train_scaled):.4f}\")\n",
    "    print(f\"  X_val_scaled 统计: 均值={np.mean(X_val_scaled):.4f}, 标准差={np.std(X_val_scaled):.4f}\")\n",
    "    print(f\"  X_test_scaled 统计: 均值={np.mean(X_test_scaled):.4f}, 标准差={np.std(X_test_scaled):.4f}\")\n",
    "    print(f\"  y_train_scaled 形状: {y_train_scaled.shape}\")\n",
    "    \n",
    "    # 计算归一化后的统计量\n",
    "    y_scaled_mean = np.mean(y_train_scaled, axis=0)\n",
    "    y_scaled_std = np.std(y_train_scaled, axis=0)\n",
    "    print(f\"  y_train_scaled 统计: 均值={y_scaled_mean[:3]}..., 标准差={y_scaled_std[:3]}...\")  # 只显示前3个\n",
    "    \n",
    "    return (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "            y_train_scaled, y_val_scaled, y_test_scaled, \n",
    "            scaler_X, scaler_y)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，用于独立运行数据处理流程并进行调试。\n",
    "    \"\"\"\n",
    "    print(\"--- [开始] 独立运行 data_processor.py ---\")\n",
    "\n",
    "    # 1. 加载和预处理数据\n",
    "    solar_data, cosmic_data = load_and_check_data()\n",
    "\n",
    "    # 2. 创建时序序列\n",
    "    X, y, dates = create_sequences(solar_data, cosmic_data)\n",
    "\n",
    "    if X.shape[0] == 0:\n",
    "        print(\"未能创建任何序列，程序终止。\")\n",
    "        return\n",
    "\n",
    "    # 3. 划分数据集\n",
    "    (X_train, X_val, X_test, \n",
    "     y_train, y_val, y_test, \n",
    "     dates_train, dates_val, dates_test) = split_data_three_way(X, y, dates)\n",
    "\n",
    "    # 4. 归一化数据\n",
    "    (X_train_scaled, X_val_scaled, X_test_scaled, \n",
    "     y_train_scaled, y_val_scaled, y_test_scaled, \n",
    "     scaler_X, scaler_y) = normalize_data_three_way(\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- [完成] 数据处理流程 ---\")\n",
    "    print(\"最终生成的数据集形状:\")\n",
    "    print(f\"  X_train_scaled: {X_train_scaled.shape}\")\n",
    "    print(f\"  y_train_scaled: {y_train_scaled.shape}\")\n",
    "    print(f\"  X_val_scaled:   {X_val_scaled.shape}\")\n",
    "    print(f\"  y_val_scaled:   {y_val_scaled.shape}\")\n",
    "    print(f\"  X_test_scaled:  {X_test_scaled.shape}\")\n",
    "    print(f\"  y_test_scaled:  {y_test_scaled.shape}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
