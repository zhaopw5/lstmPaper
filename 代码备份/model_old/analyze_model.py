#!/usr/bin/env python3
"""
æ¨¡å‹æ€§èƒ½åˆ†æå’Œæ”¹è¿›å»ºè®®è„šæœ¬
åˆ†æLSTMæ¨¡å‹çš„è®­ç»ƒç»“æœå¹¶æä¾›æ”¹è¿›å»ºè®®
"""

import sys
import os
sys.path.append('/home/phil/Files/lstmPaper_v250618/model')

import torch
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from data_processor import CosmicRayDataProcessor, split_time_series
from lstm_model import CosmicRayLSTM, ModelTrainer, calculate_metrics

# è®¾ç½®matplotlibæ˜¾ç¤º
plt.rcParams['figure.figsize'] = (12, 8)
plt.style.use('seaborn-v0_8')

def analyze_model_performance():
    """åˆ†ææ¨¡å‹æ€§èƒ½"""
    print("=== æ¨¡å‹æ€§èƒ½åˆ†æ ===")
    print("\nä»ä½ çš„è®­ç»ƒç»“æœæ¥çœ‹:")
    
    print("\n1. è®­ç»ƒé›†æ€§èƒ½ (å¾ˆå¥½):")
    print("   - RÂ² = 0.894 (89.4%çš„æ–¹å·®è¢«è§£é‡Š)")
    print("   - ç›¸å…³ç³»æ•° = 0.948 (éå¸¸å¼ºçš„æ­£ç›¸å…³)")
    print("   - RMSE = 1.92 (ç›¸å¯¹è¯¯å·®è¾ƒå°)")
    
    print("\n2. éªŒè¯é›†æ€§èƒ½ (ä¸­ç­‰):")
    print("   - RÂ² = 0.214 (ä»…21.4%çš„æ–¹å·®è¢«è§£é‡Š)")
    print("   - ç›¸å…³ç³»æ•° = 0.523 (ä¸­ç­‰ç¨‹åº¦ç›¸å…³)")
    print("   - RMSE = 2.09 (è¯¯å·®å¢åŠ )")
    
    print("\n3. æµ‹è¯•é›†æ€§èƒ½ (å·®):")
    print("   - RÂ² = -14.08 (è´Ÿå€¼è¯´æ˜æ¨¡å‹è¡¨ç°æ¯”ç®€å•å¹³å‡è¿˜å·®)")
    print("   - ç›¸å…³ç³»æ•° = -0.249 (å¼±è´Ÿç›¸å…³)")
    print("   - RMSE = 5.41 (è¯¯å·®å¾ˆå¤§)")
    
    print("\nğŸ” é—®é¢˜è¯Šæ–­:")
    print("   âŒ ä¸¥é‡è¿‡æ‹Ÿåˆ: è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½å·®è·å·¨å¤§")
    print("   âŒ æ³›åŒ–èƒ½åŠ›å·®: æ¨¡å‹æ— æ³•å¾ˆå¥½åœ°é¢„æµ‹æœªè§è¿‡çš„æ•°æ®")
    print("   âŒ æ—¶é—´åˆ†å¸ƒä¸å‡: å¯èƒ½å­˜åœ¨æ—¶é—´ç›¸å…³çš„ç³»ç»Ÿæ€§åå·®")

def suggest_improvements():
    """æä¾›æ”¹è¿›å»ºè®®"""
    print("\n=== æ”¹è¿›å»ºè®® ===")
    
    print("\nğŸ’¡ ç­–ç•¥1: è§£å†³è¿‡æ‹Ÿåˆ")
    print("   1. å¢åŠ Dropoutæ¯”ä¾‹ (0.2 â†’ 0.4)")
    print("   2. å‡å°‘æ¨¡å‹å¤æ‚åº¦ (éšè—å±‚ 128 â†’ 64)")
    print("   3. æ·»åŠ L1/L2æ­£åˆ™åŒ–")
    print("   4. å¢åŠ æ—©åœpatience")
    
    print("\nğŸ’¡ ç­–ç•¥2: æ”¹è¿›æ•°æ®å¤„ç†")
    print("   1. ä½¿ç”¨æ»‘åŠ¨çª—å£äº¤å‰éªŒè¯")
    print("   2. ç‰¹å¾å·¥ç¨‹: æ·»åŠ ç§»åŠ¨å¹³å‡ã€è¶‹åŠ¿ç­‰")
    print("   3. æ•°æ®å¢å¼º: æ·»åŠ å™ªå£°ã€æ—¶é—´æ‰°åŠ¨")
    print("   4. æ£€æŸ¥æ•°æ®è´¨é‡å’Œå¼‚å¸¸å€¼")
    
    print("\nğŸ’¡ ç­–ç•¥3: è°ƒæ•´ç½‘ç»œæ¶æ„")
    print("   1. å°è¯•å•å±‚LSTM")
    print("   2. æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶")
    print("   3. ä½¿ç”¨GRUæ›¿ä»£LSTM")
    print("   4. å°è¯•Transformeræ¶æ„")
    
    print("\nğŸ’¡ ç­–ç•¥4: ä¼˜åŒ–è®­ç»ƒç­–ç•¥")
    print("   1. é™ä½å­¦ä¹ ç‡ (0.001 â†’ 0.0001)")
    print("   2. ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦")
    print("   3. å¢åŠ è®­ç»ƒè½®æ•°")
    print("   4. è°ƒæ•´æ‰¹å¤§å°")

def create_improved_config():
    """åˆ›å»ºæ”¹è¿›çš„é…ç½®"""
    print("\n=== æ”¹è¿›é…ç½®å»ºè®® ===")
    
    configs = {
        "ä¿å®ˆæ”¹è¿›": {
            'sequence_length': 365,
            'hidden_size': 64,      # å‡å°‘å¤æ‚åº¦
            'num_layers': 1,        # å•å±‚LSTM
            'dropout': 0.4,         # å¢åŠ dropout
            'batch_size': 16,       # å°æ‰¹é‡
            'epochs': 200,          # æ›´å¤šè½®æ¬¡
            'learning_rate': 0.0001, # æ›´å°å­¦ä¹ ç‡
            'patience': 25,         # æ›´å¤šè€å¿ƒ
        },
        
        "æ¿€è¿›æ”¹è¿›": {
            'sequence_length': 180,  # å‡å°‘åºåˆ—é•¿åº¦
            'hidden_size': 32,       # æ›´å°ç½‘ç»œ
            'num_layers': 1,
            'dropout': 0.5,          # æ›´å¤šdropout
            'batch_size': 8,
            'epochs': 300,
            'learning_rate': 0.00005,
            'patience': 30,
        }
    }
    
    for name, config in configs.items():
        print(f"\n{name}é…ç½®:")
        for key, value in config.items():
            print(f"   {key}: {value}")

def plot_data_analysis():
    """åˆ†ææ•°æ®åˆ†å¸ƒ"""
    print("\n=== æ•°æ®åˆ†æ ===")
    
    # æ•°æ®è·¯å¾„
    solar_data_path = '/home/phil/Files/lstmPaper_v250618/data/solar_physics_data_1980_extend_smoothed.csv'
    cosmic_ray_path = '/home/phil/Files/lstmPaper_v250618/data/raw_data/ams/helium.csv'
    
    try:
        # åŠ è½½æ•°æ®
        processor = CosmicRayDataProcessor(solar_data_path, cosmic_ray_path)
        processor.load_data()
        
        # åˆ†æå®‡å®™çº¿æ•°æ®åˆ†å¸ƒ
        cosmic_flux = processor.cosmic_data['helium_flux m^-2sr^-1s^-1GV^-1']
        
        plt.figure(figsize=(15, 10))
        
        # æ—¶é—´åºåˆ—å›¾
        plt.subplot(2, 2, 1)
        plt.plot(processor.cosmic_data['date YYYY-MM-DD'], cosmic_flux)
        plt.title('Cosmic Ray Flux Time Series')
        plt.xlabel('Date')
        plt.ylabel('Flux')
        plt.xticks(rotation=45)
        
        # ç›´æ–¹å›¾
        plt.subplot(2, 2, 2)
        plt.hist(cosmic_flux, bins=50, alpha=0.7)
        plt.title('Cosmic Ray Flux Distribution')
        plt.xlabel('Flux')
        plt.ylabel('Frequency')
        
        # ç»Ÿè®¡ä¿¡æ¯
        plt.subplot(2, 2, 3)
        stats_text = f"""
        ç»Ÿè®¡ä¿¡æ¯:
        Mean: {cosmic_flux.mean():.2f}
        Std: {cosmic_flux.std():.2f}
        Min: {cosmic_flux.min():.2f}
        Max: {cosmic_flux.max():.2f}
        Skewness: {cosmic_flux.skew():.2f}
        """
        plt.text(0.1, 0.5, stats_text, fontsize=12, transform=plt.gca().transAxes)
        plt.axis('off')
        
        # è¶‹åŠ¿åˆ†æ
        plt.subplot(2, 2, 4)
        # è®¡ç®—30å¤©ç§»åŠ¨å¹³å‡
        cosmic_flux_ma = cosmic_flux.rolling(window=30).mean()
        plt.plot(processor.cosmic_data['date YYYY-MM-DD'], cosmic_flux, alpha=0.3, label='Original')
        plt.plot(processor.cosmic_data['date YYYY-MM-DD'], cosmic_flux_ma, label='30-day MA')
        plt.title('Trend Analysis')
        plt.xlabel('Date')
        plt.ylabel('Flux')
        plt.legend()
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        save_path = '/home/phil/Files/lstmPaper_v250618/model/results/data_analysis.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"æ•°æ®åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        
        # æ£€æŸ¥æ•°æ®å¼‚å¸¸
        q1 = cosmic_flux.quantile(0.25)
        q3 = cosmic_flux.quantile(0.75)
        iqr = q3 - q1
        outliers = cosmic_flux[(cosmic_flux < q1 - 1.5*iqr) | (cosmic_flux > q3 + 1.5*iqr)]
        
        print(f"\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(cosmic_flux)}")
        print(f"   å¼‚å¸¸å€¼æ•°é‡: {len(outliers)} ({len(outliers)/len(cosmic_flux)*100:.1f}%)")
        print(f"   æ•°æ®å˜å¼‚ç³»æ•°: {cosmic_flux.std()/cosmic_flux.mean()*100:.1f}%")
        
    except Exception as e:
        print(f"æ•°æ®åˆ†æå¤±è´¥: {e}")

def create_improved_model_script():
    """åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹è„šæœ¬"""
    print("\n=== åˆ›å»ºæ”¹è¿›æ¨¡å‹ ===")
    
    script_content = '''#!/usr/bin/env python3
"""
æ”¹è¿›çš„LSTMæ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºç¬¬ä¸€æ¬¡è®­ç»ƒçš„ç»éªŒè¿›è¡Œä¼˜åŒ–
"""

import torch
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os
import sys

sys.path.append('/home/phil/Files/lstmPaper_v250618/model')

from data_processor import CosmicRayDataProcessor, split_time_series
from lstm_model import CosmicRayLSTM, ModelTrainer, calculate_metrics

def create_data_loaders(X_train, y_train, X_val, y_val, batch_size=16):
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    return train_loader, val_loader

def main_improved():
    """æ”¹è¿›çš„ä¸»è®­ç»ƒæµç¨‹"""
    print("=== æ”¹è¿›çš„å®‡å®™çº¿LSTMæ¨¡å‹ ===")
    
    # æ”¹è¿›çš„è¶…å‚æ•°
    config = {
        'sequence_length': 365,
        'hidden_size': 64,        # å‡å°‘å¤æ‚åº¦
        'num_layers': 1,          # å•å±‚LSTM
        'dropout': 0.4,           # å¢åŠ æ­£åˆ™åŒ–
        'batch_size': 16,         # å°æ‰¹é‡
        'epochs': 200,            # æ›´å¤šè½®æ¬¡
        'learning_rate': 0.0001,  # æ›´å°å­¦ä¹ ç‡
        'patience': 25,           # æ›´å¤šè€å¿ƒ
    }
    
    print("æ”¹è¿›é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # æ•°æ®å¤„ç† (ä¸åŸæ¥ç›¸åŒ)
    solar_data_path = '/home/phil/Files/lstmPaper_v250618/data/solar_physics_data_1980_extend_smoothed.csv'
    cosmic_ray_path = '/home/phil/Files/lstmPaper_v250618/data/raw_data/ams/helium.csv'
    
    processor = CosmicRayDataProcessor(
        solar_data_path=solar_data_path,
        cosmic_ray_path=cosmic_ray_path,
        sequence_length=config['sequence_length'],
        target_rigidity=2.97
    )
    
    X, y, dates = processor.prepare_data()
    
    # æ—¶é—´åºåˆ—åˆ†å‰²
    (X_train, y_train, dates_train), (X_val, y_val, dates_val), (X_test, y_test, dates_test) = \\
        split_time_series(X, y, dates, train_ratio=0.7, val_ratio=0.15)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_data_loaders(
        X_train, y_train, X_val, y_val, 
        batch_size=config['batch_size']
    )
    
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    model = CosmicRayLSTM(
        input_size=X.shape[2],
        hidden_size=config['hidden_size'],
        num_layers=config['num_layers'],
        dropout=config['dropout']
    )
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    trainer = ModelTrainer(model, device=device)
    
    # è®­ç»ƒ
    output_dir = '/home/phil/Files/lstmPaper_v250618/model/results'
    model_save_path = os.path.join(output_dir, 'improved_model.pth')
    
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        patience=config['patience'],
        save_path=model_save_path
    )
    
    # è¯„ä¼°
    checkpoint = torch.load(model_save_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_test_pred = trainer.predict(X_test)
    y_test_true = processor.inverse_transform_y(y_test.numpy())
    y_test_pred = processor.inverse_transform_y(y_test_pred)
    
    test_metrics = calculate_metrics(y_test_true, y_test_pred)
    print("\\næ”¹è¿›æ¨¡å‹æµ‹è¯•é›†æ€§èƒ½:")
    for metric, value in test_metrics.items():
        print(f"  {metric}: {value:.6f}")
    
    return model, processor, trainer, test_metrics

if __name__ == "__main__":
    model, processor, trainer, metrics = main_improved()
'''
    
    save_path = '/home/phil/Files/lstmPaper_v250618/model/train_improved_lstm.py'
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"æ”¹è¿›æ¨¡å‹è„šæœ¬å·²åˆ›å»º: {save_path}")

def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("ğŸ” LSTMå®‡å®™çº¿é¢„æµ‹æ¨¡å‹ - æ€§èƒ½åˆ†æä¸æ”¹è¿›å»ºè®®")
    print("="*60)
    
    # åˆ†æå½“å‰æ€§èƒ½
    analyze_model_performance()
    
    # æä¾›æ”¹è¿›å»ºè®®
    suggest_improvements()
    
    # é…ç½®å»ºè®®
    create_improved_config()
    
    # æ•°æ®åˆ†æ
    plot_data_analysis()
    
    # åˆ›å»ºæ”¹è¿›è„šæœ¬
    create_improved_model_script()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’:")
    print("1. è¿è¡Œæ”¹è¿›çš„æ¨¡å‹: python train_improved_lstm.py")
    print("2. æ¯”è¾ƒæ–°æ—§æ¨¡å‹æ€§èƒ½")
    print("3. æ ¹æ®ç»“æœè¿›ä¸€æ­¥è°ƒä¼˜")
    print("4. è€ƒè™‘æ›´é«˜çº§çš„æŠ€æœ¯ (æ³¨æ„åŠ›æœºåˆ¶, Transformerç­‰)")

if __name__ == "__main__":
    main()